<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>it 鐵人賽 2018 on 拿鐵派的馬克 Blog</title>
    <link>https://mark-lin.com/tags/it-%E9%90%B5%E4%BA%BA%E8%B3%BD-2018/</link>
    <description>Recent content in it 鐵人賽 2018 on 拿鐵派的馬克 Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <managingEditor>h091237557@gmail.com (marklin)</managingEditor>
    <webMaster>h091237557@gmail.com (marklin)</webMaster>
    <lastBuildDate>Sun, 30 Sep 2018 19:51:35 +0800</lastBuildDate>
    
        <atom:link href="https://mark-lin.com/tags/it-%E9%90%B5%E4%BA%BA%E8%B3%BD-2018/index.xml" rel="self" type="application/rss+xml" />
    


    <item>
      <title>30-30之寫給想入門影音直播開發的 Junior 工程師攻略本</title>
      <link>https://mark-lin.com/posts/20180930/</link>
      <pubDate>Sun, 30 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180930/</guid>
      <description>前言 這辛苦的三十天總於結束了，當初原本只是想要理解直播相關協議的東西一下，但是卻發現越來越多奇怪的名詞跑了出來，就算大概的知道 HLS 是要做什麼，但你總是會覺得前後知識好像缺了什麼，別人問你一下沒有 HLS 前是如何處理直播傳輸你大概就會倒了，然後上網查查相關知識，但卻發現總是沒有連貫起來的知識，越看越冒出更多的名詞，這對一名想入門影音直播開發的初學者，真的非常的辛苦。
因此最後就想說當個好人將這方面的知識都整理成一條線好了，因此就產生了這三十篇的文章。
這三十天的文章基本上理解完後，要開發出一個直播應用事實上就不是太難的事情了，而且你有了這三十篇的知識你接下來要將直播應用開發的更好，你就也更有本錢去學習更進階的東西囉。
接下來這篇文章就來總結一下這三十天咱們學習了那些東西。
影音直播開發的 Junior 工程師攻略本 基本上前二十篇是最基礎的東西，這幾篇文章就在說明，如何將一個人的聲音或影像傳遞給對方。。
!
聲音與影像的採集、編碼與封裝 首先這五篇文章我們學習了聲音與影像是如何的儲放在電腦裡面，並且因為原始的聲音與影像太大，所以我們需要使用編碼來進行壓縮，最後就準備需要將聲音與影像進行封裝，為了可以讓別人一打開來知道要如何處理這一段聲音或影像編碼。
 30-02之聲音的採集與原理 30-03之聲音的編碼與壓縮 30-04之影像的採集與原理 30-05之影像的編碼與壓縮 30-06之聲音與影像的封裝  接下來的兩篇文章，咱們就使用 WebRTC 的一些東西，來實作如何的採集聲音與影像，並且最後將它封裝成一個檔案。
 30-07之Web 如何進行語音與影像採集 ? 30-08之 WebRTC 採集的詳細說明與聲音的加工  聲音與影像的傳輸 咱們已經可以將聲音與影像封裝後，就可以開始傳輸給遠方的某個人。
我們一開始先探討讓對方收到聲音與影像的方法有那些。
 30-09之別人要如何聽到我的聲音呢 ?  接下來花了幾篇文章來理解網路傳輸協議。
 30-10之通訊協議的基本常識 30-11之 TCP 與 UDP 協議 30-12之 RTP/RTCP 傳輸協議 30-13之 RTSP 傳輸協議 30-14之 RTMP 傳輸協議 30-15之 HLS 傳輸協議 30-16之 HTTP-FLV 傳輸協議 30-17之 MPEG-DASH 傳輸協議 30-18之影音傳輸協議總整理  在理解完上面幾篇文章以後，我們應該是可以將聲音與影像傳輸給遠方的某個人。</description>
    </item>
    
    <item>
      <title>30-29之 WebRTC 的 P2P 打洞術 ( ICE )</title>
      <link>https://mark-lin.com/posts/20180929/</link>
      <pubDate>Sat, 29 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180929/</guid>
      <description>正文開始 上一篇文章中，咱們已經理解了為什麼 P2P 連線如此的困難，接下來這篇文章咱們將要學習：
 WebRTC 是如何進行打洞與連線呢 ?
 WebRTC 的打洞流程 ICE WebRTC 它主要使用一個名為ICE ( Interactive Connectivity Establishment ) 的框架來進行打洞，它內部整合了 STUN 與 TURN 協議，下面簡單的說明一下這兩個協議。
STUN ( Session Traversal Utilities for NAT ) 中文為 NAT 對談穿透應用程式，它的最主要用處就是幫助在 NAT 內的用戶找到可以連到它的位置。
STUN-RFC3489 STUN-RFC5389
TURN ( Traversal Using Relay NAT )，它也是一種穿透 NAT 的一樣協議，不過它是使用中繼的方式來進行，通常都是 STUN 的候選位置都無法連線時，才會使用它。
TURN-RFC5766
WebRTC 連線流程 假設目前要連線的雙方情況如下：
A 內網位置：192.168.1.1:5555 B 內網位置：10.10.1.1:7777 A 外網位置(經過 NAT 轉換)：310.110.1.1:9000 B 外網位置(經過 NAT 轉換)：210.210.1.1:7000 TURN Server：111.111.111.111 下圖為示意圖。表示雙方去外部連 Server 時對外的的位置。</description>
    </item>
    
    <item>
      <title>30-28之 WebRTC 連線前傳 - 為什麼 P2P 連線很麻煩 ? ( NAT )</title>
      <link>https://mark-lin.com/posts/20180928/</link>
      <pubDate>Fri, 28 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180928/</guid>
      <description>正文開始 在開始說明 WebRTC 如何建立 P2P 連線前，咱們要先理解一件事情，那就是 WebRTC 要使用非常多的 P2P 連線技術，那位啥它需要使用如此多的技術呢 ? 那就是本篇文章要探討的主題：
 為什麼 P2P 連線很麻煩呢 ?
 因為如果你理解了這個問題，你就會知道為什麼 WebRTC 要使用怎麼多的技術來進行 P2P 連線囉，這也是為什麼我們會先說明這篇文章。
本篇文章的問題，為什麼 P2P 的連線會很麻煩呢 ? 最主要的問題在於：
 NAT 與防火牆的存在。
 因此本篇文章將針對這兩個東西來理解
 NAT 與防火牆是啥 ? NAT 的運作原理。 為什麼有了 NAT 後 P2P 會很麻煩 ? NAT 的分類。  NAT 與防火牆是啥 ? NAT NAT (Network Address Translation) 中文就做網路位置轉換，它是用來將私網 IP 轉換成公網 IP 的技術。
為啥會有 NAT ? 先說說它的起源。
在咱們世界裡有個叫 IPV4 的地址規則，由於它數量稀少，不可能讓每一台電腦都有一個地址，因此就有了以下的解法如下圖，就是每個家庭或公司只有一組地址，然後公司內的電腦就使用這位置來上網。
其中常用的私有 IP 段為：</description>
    </item>
    
    <item>
      <title>30-27之 WebRTC 的 Signaling Server</title>
      <link>https://mark-lin.com/posts/20180927/</link>
      <pubDate>Thu, 27 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180927/</guid>
      <description>正文開始 上一篇文章中，咱們已經學習完了 WebRTC 的一些基本知識，接下來咱們要針對 Signaling Server 這東東來做做一些比較深的的探討。
 Signaling Server 要做的事情。 WebRTC 與 SDP。 Signaling Server的實作選擇。  Signaling Server 要做的事情 在上一篇文章，我們大概知道了 Signaling Server 要做的事情，它要做的就是：
 在建立 WebRTC 時，讓不認識的雙方可以相互的認識 (也就是知道對方的位置)
 順到說一下 WebRTC 並沒有定議 Signaling Server 的標準。
那 Signaling 是如何讓雙方知道對方的位置呢 ? Signaling 是如何讓雙方認識呢 ?
它就像是一個交友仲介商，每當某位用戶要進行聯誼時，用戶會提交一份履歷，這裡面就寫這他家在那，然後有配對到時，就會將這份履歷交給配對者，然後配對者如果覺得可以認識看看，他也會提供一份履歷給 Signaling Server，這樣雙向就可以進行面談了，其中已 Signaling 角度而言，履歷就是指 SDP。
什麼是 SDP ? 下章節在說。
為啥 WebRTC 不建立 Signaling Server 的標準呢 ? 因為事實上要讓兩個瀏覽器能進行溝通，可以不需要 Signaling Server，如果你知道對方在那的情況下，在筆者的『30-10之通訊協議的基本常識』這篇文章中有提到，假設你知道了對方的 port 就代表你可以找到對方電腦內某個應用程式的位置，而你知道了 ip 那就代表你知道對方在那，所以這時你事實上就可以與對方溝通。
WebRTC 與 SDP SDP (Session Description Protocol) 中文叫會話描述協議，在一段會話建立起來前，咱們需要一些建立這會話雙方的資訊，假設 A 與 B 要建立會話，所以這時 A 會發送一個 SDP 給 Signaling，內容包含了 A 的地址、媒體類型、傳輸協議、媒體格式等或是一些它所在的時區資訊，然後 Signaling 會將 A 發送的 SDP 給 B，這樣雙方就知道如何建立連線了。</description>
    </item>
    
    <item>
      <title>30-26之 WebRTC 的 P2P 即時通信與小範例</title>
      <link>https://mark-lin.com/posts/20180926/</link>
      <pubDate>Wed, 26 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180926/</guid>
      <description>正文開始 在很前面的文章中，咱們有簡單的介紹如何使用 WebRTC 來採集聲音與影像，但那時只是很簡單的介紹一下而以，所以接下來的幾篇文章，咱們將要來深入的了解 WebRTC。
這篇文章將要介紹幾個 WebRTC 的基概念，大約分成以下幾個章節:
 WebRTC 的誕生與內部架構。 WebRTC 所支援的語音視編碼與傳輸協議。 WebRTC 提供的基本 P2P 功能。 WebRTC 的簡單通訊實作。  WebRTC 的誕生與內部架構 首先在 Web 通信的世界中，基本上都是所謂的 C/S 架構，也就是所謂的 client 與 server 架構，通常 client 要取資料時就是發送一個請求給 server 然後它會回傳資料回去，其中 ajax 的出現讓我們更能以少量的資源來取得資料，在這階段時都部份都是單向溝通，也就是 client 請求 server。
而在二階段能，人們開始有種需求，例如股票報價網站，人們希望可以看到當有股價變動時，網頁可以也同時更動，這時如果用上面那種模式，那就只能 client 定時的去 server 拿資料，也就是咱們所謂的輪詢，但這種方法很明顯的非常的浪費資源，你可以呼叫 server 十次，但只有一次才真的有新的資料。而這時webSocket就用來解決這向事情，它提供了雙向溝通功能，server 就可以透過它，來將資料推給 client。
基本上以上已經解決了 client 與 server 的雙向互動，但這時人們又在想，假設我是做個一對一的聊天工具，那為什麼還需要 server 呢? 不能直接 client 與 client 進行溝通就好呢 ? WebRTC 就是可以幫助我們完成的工具，它就是用來專門處理瀏覽器與瀏覽器之間的即時溝通。
備註：雖然說是 cleint 與 client 直接進行溝通，但不是說不需要 server，後面會說明。</description>
    </item>
    
    <item>
      <title>30-25之直播連麥的挑戰與方案 ( P2P )</title>
      <link>https://mark-lin.com/posts/20180925/</link>
      <pubDate>Tue, 25 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180925/</guid>
      <description>正文開始 現今大部份的直播咱們要可以與漂亮的直播主姐姐或硬漢大叔進行互動，基本上咱們只能使用文字，也就是所謂的聊天室，而這篇文章咱們將要介紹另一種互動方式，那就是直播連麥，也就是直播主與聽眾可以進行語音溝通，更白話文的是說可以和漂亮姐姐進行語音聊天。這也就是本篇文章的主題。
 可以和漂亮姐姐進行語音聊天的難題與方案。
 本篇文章將分成兩個章節：
 直播連麥的挑戰 直播連麥的架構  直播連麥的挑戰  直播連麥最大的難題就是『 延遲 』問題。
 假設我們在一般直播時，以比較低的延遲 2 秒來計算的話 ( 就是主播說話聽眾 2 秒鐘後才能聽到 )，那這樣在直播連麥時會發生什麼事情呢 ?
如下圖所示，直播主說話以後，要經過 4 秒以後才會聽到連麥者的回復，而連麥者也相同的要等 4 秒後才能聽到直播主的回應，你覺得這樣還可以對話嗎 ?
這裡來問個問題。
為什麼直播會延遲呢 ? 基本上可以分四個部份，如下圖。呃下面兩個箭頭算一部份，所以分別為直播主處理、網路傳輸、CDN 與 Media Server 處理、聽眾端處理。
首先第一個部份是直播主 Client 它需要花時間來進行聲音與影像的採集，接下來進行編碼，最後就準備封裝然後準備送貨。
這部份有沒有可以優化的地方呢 ?
基本上可以在編碼上加速，網路上有提到說硬編碼與軟編碼這兩個聽起來很硬東西，其中軟編碼就是使用 CPU 來編碼，而硬編碼就是使用非 CPU 來進行編碼，例如使用 GPU。
而硬編碼效能優於軟編碼，不過需要硬體支援，這方面我沒很熟，請當參考。
第二部份為網路傳輸 只要實用網路進行傳輸，基本上都一定會發生延遲問題，而最主要產生的原因有兩個：
 距離。 傳輸時的封包維護。  首先距離是不用說的，直播主離你越遠，它的聲音要傳到你的小耳朵裡，一定比較花時間。
然後第二點是傳輸時的封包維護，在網路傳輸時基本上會發生兩件事情網路抖動與網路掉包。
網路抖動就是你傳送 A、B、C 封包給某個人，但某個人收到的封包為 C、B、A，那這時就要針對這狀態做一些處理，方法可能是緩存等到某段時間後，在根據封包裡面的時間來依順序播放，但緩存那邊就會產生延遲時間囉。
而網路掉包就是你傳送 A、B、C 但實際上對方只收到 A、B，而這時如果是 TCP 的話，它就會一段時間後，會在發送一次封包 C，然後這段時間也就產生延遲囉。</description>
    </item>
    
    <item>
      <title>30-24之直播與點播可動版的改良 ( 正式版 )</title>
      <link>https://mark-lin.com/posts/20180924/</link>
      <pubDate>Mon, 24 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180924/</guid>
      <description>正文開始 上一篇文章中咱們學習完了 CDN 的相關知識以後，接下來這篇文章，我們將要將上一篇所學的來改善咱們以下兩篇文章可動版的架構。
 使用 CDN 來調整可動版的架構
 30-20之如何建立像 KKTV 一樣的點播功能呢 ? 30-21之如何建立的像 17 一樣的直播功能呢 ?
點播 直播 本篇文章會分成以下幾個章節：
 可動版本的問題。 點播的架構改善版本。 直播的架構改善版本。  可動版本的問題 在筆者這篇文章中『30-22之點播與直播可動版問題探討』，我們探討了可動版有以下的問題：
 人多時連線數限制問題。 人多時頻寬問題。 人多時效能消耗問題。 遠距離看片卡頓的問題。  基本上 1、3 我們可以用傳統的方法(加機器)來解決，而 2、4 就無法使用加機器來進行解決，因為以 2 的頻寬問題，就算你加了在多的機器，如果你在出去的網路還是在同一條，那頻寬還是沒加大，問題還是沒解決。而 4 的話就更不用說，只能讓使用者離機器更近一點才能解決。
點播架構的改善版本 基本上點播的架構會改成如下圖，它會將 CDN 加上去，而這樣就可幫助我們解決頻寬與距離看片卡頓的問題。
先來說說頻寬的部份， CDN 基本上分散在不同的地方，這也代表這基本上它們的頻寬是各別獨立的，所以不太會發生搶頻寬的問題，而另一點距離的問題，由於大部份的 CDN 都會搭配使用智能 DNS 來幫助找到最近的 CDN，因此可以解決因為離影片來源太遠，而容易造成封包遺失與封包順序不一致問題。
而且用了 CDN 事實上還有一個好處，那就是可以保證接近 100 的機率可以看片，你想想如果是自已建的一台 Media Server 來提供看片，如果它倒了，不就不能看片了，而有 CDN 就可以自動的轉到另一個臨近的 CDN 來取得資料。
接下來說說它的運行流程，基本上如下 (以 HLS 拉流為範例)：</description>
    </item>
    
    <item>
      <title>30-23之 CDN 的說話島 ( AWS CloudFront  CDN 實作 )</title>
      <link>https://mark-lin.com/posts/20180923/</link>
      <pubDate>Sun, 23 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180923/</guid>
      <description>正文開始 上一篇文章中，咱們有提到點播與直播可動版本的一些問題，如下所列。
 人多時連線數限制問題。 人多時頻寬問題。 人多時效能消耗問題。 遠距離看片卡頓的問題。  接下來咱們要來理解一下，解這上述問題的關鍵技術 CDN 是什麼東西 ?
本編文章將分為以下幾個章節：
 什麼是 CDN 呢 ? 它又是用來解決啥呢 ? CDN 的請求運作方式。 使用 AWS CloudFront 來建立 CDN。  什麽是 CDN 呢 ? 它又是用來解決啥呢 ? 在開始理解 CDN 之前，咱們先來說說傳統上一個 client 連線到一個網站的流程。
首先看看下面這張圖，這張圖說明了每當一個 client 發送一個請求到 web 網站時，web 網站會回傳 html、css 與 javascript 回來，這裡假設咱們的 web 網站還在台灣，然後回應時間大約在 100 ms 以內 (假設)。
然後呢 ~ 這時付你錢的老大叫你將 web 網路架設到美國，因爲免費，然後這時發現回應時間變成 1000 ms 左右。
然後開始了有以下的對話 :
老大：回應時間怎麼回事 ? 碼農仔：老大你叫我架到美國啊 !? 老大：我要的不是這回答，而是問你為啥回應時間你沒修改回來 ?</description>
    </item>
    
    <item>
      <title>30-22之點播與直播可動版問題探討</title>
      <link>https://mark-lin.com/posts/20180922/</link>
      <pubDate>Sat, 22 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180922/</guid>
      <description>正文開始 前面文章中咱們簡單了可以動的點播 ( like KKTV )與直播 ( like 17 )的功能，那接下來這篇文章主題要探討的目問題為：
 這兩篇文章實際上應用會有什麼問題 ?
 30-20之如何建立像 KKTV 一樣的點播功能呢 ? 30-21之如何建立的像 17 一樣的直播功能呢 ?
點播可動版架構 直播可動版架構 本篇文章分根據問題分成以下幾個章節：
 人多時連線數限制問題。 人多時頻寬問題。 人多時效能消耗問題。 遠距離看片卡頓的問題。  人多時連線數限制問題 咱們都知道，每當 Client 要和 Server 要資料時，如果是以 TCP 傳輸為基礎的，那就一定要建立一條連線，那對 Server 而言連線是啥了 ?
在 unix 中每一個 TCP 連線都要占用一個 file descriptor，而它有一定的限制數量，當使用完後，新的 TCP 連線到來就會發生錯誤以下的錯誤訊息 :
Socket/File: Can&#39;t open so many files。 那一個 process 我們可以開啟幾個檔案呢 ? 我們可以用以下的指令來看看 :
ulimit -n 那這個最大值為多少呢 ? 這個我不確定，像我家用的 Aws Elasticsearch 就有 128000。</description>
    </item>
    
    <item>
      <title>30-21之如何建立的像 17 一樣的直播功能呢 ?</title>
      <link>https://mark-lin.com/posts/20180921/</link>
      <pubDate>Fri, 21 Sep 2018 19:51:35 +0800</pubDate>
      <author>h091237557@gmail.com (marklin)</author>
      <guid>https://mark-lin.com/posts/20180921/</guid>
      <description>正文開始 上一篇文章中，咱們已經學習了如何建立點播這種類型的網站應用，接下咱們要來學學如何建立直播應用。
在筆者的30-09之別人要如何聽到我的聲音呢 ?有提到三種影音的傳遞方式，分別為：
將聲音檔案直接丟給對方 ( 方法 1 ) 將聲音檔案以串流的方式傳送給對方 ( 方法 2 ) 像直播或網路電話一樣即時的將聲音傳送給對方 ( 方法 3 )
接下來我們將來實作方法 3 的選項，而這東西事實上就是直播網站的應用，像 17 就是這種類型的應用，本篇文章將會說明：
 如何建立的像 17 一樣的直播功能呢 (可以動就好版) ?
 本篇將分為以下幾個章節：
 直播架構原理。 實作 - 建立 Media Server。 實作 - 網頁用戶端取得串流影像。  直播架構原理 直播的架構最基本的如下圖，基本上和點播很相似，只差了直播主推送聲音到 Media Server 這個步驟。
然後基本上推流的協議，應該是只有一種選擇RTMP，網路上看到有人說 HLS 這點我還要待調查，我先打個問號， 然後還有提到 WebRTC 這個可以當推流 (不過它應該不算傳輸協議)，這理論上應該是行，這之後 WebRTC 的文章會來聊聊。
而拉流就有不少RTMP、HLS、HTTP-FLV、MPEG-DASH。
老樣子問個問題。
那要選擇那個協議來當拉流呢 ? 首先我覺得要先看你的直播應用是否為互動性很要求的，如果是它就只有兩個選擇 RTMP 與 HTTP-FLV。不過 RTMP 拉流應該會有不少設備無法使用(果粉)，所以最後應該會選 HTTP-FLV 吧。
而如果是不會太在意互動性的直播，例如運動賽事直播這種，那選那個，我覺得就看用戶那的支援度吧。</description>
    </item>
    
  </channel>
</rss>